% !TeX encoding = UTF-8
\section{Method}

In this chapter, I will give a brief overview over Grounded Theory Methodology (GT), the main theory which inspires this
thesis, while the next chapter describes my application. GT has a rich history with various evolutions and branches
emerging over the last few decades. \cite{glaser_discovery_2017, corbin_basics_2015, charmaz_constructing_2014}
While all of them share a common root, they can emphasize different points to varying
degrees, or even disagree on some of the fundamentals. This summary is supposed to facilitate understanding for the later
chapters while introducing important terminology.

\subsection{Data Collection}

At the core of classic Grounded Theory is its distinctive approach to data collection. First introduced in the field of
medical research by its founders, Barney Glaser and Anselm Strauss, Grounded Theory is described as a "way of arriving
at theory suited to its purposed use" in contrast to theories "generated by logical deduction from a priori assumptions"
\cite[ p.3]{glaser_discovery_2017}. While other research methods would start with a clearly defined research goal, Glaser and
Strauss advocate for minimal preconceived notions about the research subject.

Despite tensions within the development of Grounded Theory in the following decades, the essence of GT
remains unchanged: the researcher should systematically engage with the data with an open mind, actively seeking
connections to the broader phenomena under investigation, while being guided by the data and not by any \"opportunistic
use of theory" \cite[ p.5]{glaser_discovery_2017}.\cite{niasse_limiting_2023}

GT driven data collection is characterized by its methodological flexibility, allowing for a broad range of qualitative
data types - such as interviews, observations, and document analysis - to be employed based on the evolving nature of
the inquiry. The primary goal is to gather rich, detailed data that reflects the complexities of the phenomena being studied.

\subsection{Encoding Data}

The practice of encoding\footnote[1]{This thesis uses “encoding” where GT would usually use “coding” in order to have a
clear distinction between coding as a research method, and coding as in creating computer programs.}
is significantly shaped by Barney Glaser's experiences at Paul Lazarsfeld's institute during
the 1950s. Adopted from quantitative content analysis in social research, this approach is central for developing
categories. \cite[p.69-70]{bryant_sage_2019} Encoding data means ascribing meaning to data, progressing from specific details to
so-called axis, overarching themes, and ultimately, to a cohesive theory.

The term took a pivot in the 1990s with a more constructivist approach proposed by Kathy Charmaz. This approach asserts
encoding as an iterative and reflexive process, as “our assumptions, interactions—and interpretations—affect the social
processes constituting each stage of inquiry.” \cite[p.132]{charmaz_constructing_2014} This means that when new or different data is
required, it can be added freely. When new specific phenomena emerge from the data, they should be captured as such and
not coerced into existing categories, the categories and codes should be updated accordingly.

Reflexivity in this context refers to the researchers ability to critically reflect on their own role, potential biases
and influences during the research process. Reflexivity ensures that the researcher remains open to the data, allowing
theories to emerge from the data itself, rather than imposing preconceived notions. As such it is crucial in constructing
a theory that it is grounded in the empirical data, instead of imposed on it.

Unlike many quantitative methods aimed at representativeness, in GT a phenomenon can exist with only one example.
This of course does not mean it is a general phenomenon: To be able to generalize from the data, the research needs to
reach theoretical saturation, that is, looking at new data "no longer sparks new theoretical insights, nor reveals new
properties of these core theoretical categories". \cite[p.113]{charmaz_constructing_2014} At this point, further data analysis and
category building can be conducted in order to finalize a theory.

Open Coding is the initial phase of data analysis in GT. The researcher begins to break down, examine, compare and
categorize data. Oftentimes, this is done line-by-line, or even word-by-word. Codes can be highly specific, and sometimes
describe observed phenomena almost word for word the way they are seen in the data. A hallmark of open coding is the
constant comparative method. As Charmaz describes, it involves an interpretive interaction between researchers and their
data. Through this method, researchers identify similarities and differences, may group them together, and recognize
possible relations among them. The purpose is to "make sense of our situations, appraise what occurs in them, and draw
on language and culture to create meanings and frame actions." \cite[p.179]{charmaz_constructing_2014}

Another important part of open coding, emphasized to varying degrees by different practitioners, is Memo-Writing,
where researchers note down insights, hypotheses, questions and reflections about the data and emerging analysis.
Memos play an important role in capturing the analytical process and facilitating the development of a cohesive and
grounded theoretical framework.

As mentioned before, representativeness of the data is not a concern here, which means it is good for the researcher
to actively seek out the most promising data. This ability of the researcher to find the right data, and notice the
right phenomena is often called theoretical sensitivity.

The core task for axial coding, which refers to the process of “ Crosscutting or relating concepts to each other”\cite[p.195]{corbin_basics_2015},
lies in developing categories by joining codes that fit together and linking data
explicitly, identifying possible causal relationships, mapping out processes and understanding how different categories
interact within specific context. This step is iterative as well, requiring constant
movement between data and emerging theory to refine the understanding of these relationships.

At this point, the core categories explaining the research subject can be identified. They are the most significant or
prevalent categories, around which all other categories can be related. Selective coding involves the integration of
categories around the core category to form a coherent theoretical framework. Frequently researchers here will actively
seek out data eg. by using key-word searches or other heuristics to fill gaps in the relations of the categories.
The goal is to reach theoretical saturation.

In essence axial coding and selective coding are iterative and interactive processes that move the researcher from
identifying relationships between categories to integrating these categories into a coherent theoretical framework,
which not only explains the core phenomena, but also places them within a broader context and explains their implications,
variations, and limitations.
